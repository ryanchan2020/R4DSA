filter(spam_check > 1.1) %>%
plot_ly(x=~author,y=~spam_check, type = "bar")
datatable(mb_messages_by_author_aggregated)
mb_messages_by_author_aggregated %>%
filter(spam_check > 1.1) %>%
plot_ly(x=~author,y=~spam_check, type = "bar") %>%
layout(y_axis=list(categoryorder = "category descending"))
spam_authors_ordered <- mb_messages_by_author_aggregated$author
mb_messages_by_author_aggregated %>%
filter(spam_check > 1.1) %>%
plot_ly(x=~author,y=~spam_check, type = "bar") %>%
layout(y_axis=list(categoryorder = "array", categoryarray=spam_authors_ordered))
mb_messages_by_author_aggregated %>%
filter(spam_check > 1.1) %>%
plot_ly(x=~author,y=~spam_check, type = "bar") %>%
layout(yaxis=list(categoryorder = "array", categoryarray=mb_messages_by_author_aggregated$author))
mb_messages_by_author_aggregated %>%
filter(spam_check > 1.1) %>%
plot_ly(x=~author,y=~spam_check, type = "bar") %>%
layout(xaxis=list(categoryorder = "array", categoryarray=mb_messages_by_author_aggregated$author))
datatable(mb_messages_by_author_aggregated)
mb_messages_by_author_aggregated %>%
filter(spam_check > 1.1) %>%
plot_ly(x=~author,y=~spam_check, type = "bar") %>%
layout(xaxis=list(categoryorder = "array", categoryarray=mb_messages_by_author_aggregated$author))
mb_cleaned <- mb[mb$author!="KronosQuoth" & mb$author!="Clevvah4Evah"]
mb_words <- mb_cleaned %>%
unnest_tokens(word, cleaned_message, drop = FALSE) %>%
filter(str_detect(word, "[a-z']$"),
!word %in% stop_words$word)
View(mb_words)
mb_words <- select(mb_words, -c(type))
mb_words_by_author <- mb_words %>%
count(author, word, sort = TRUE) %>%
ungroup()
mb_words_by_author$in_dictionary <- if_else (mb_words_by_author$word %in% GradyAugmented,0,1)
mb_words_by_author$sentiment <- get_sentiment(mb_words_by_author$word, method = "syuzhet")
mb_words_by_author$sentiment <- get_sentiment(mb_words_by_author$word, method = "syuzhet")
mb_words_by_author$total_sentiment <- mb_words_by_author$sentiment*mb_words_by_author$n
View(mb_words_by_author)
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
group_by(author) %>%
summarise(grandtotal_sentiment = sum(total_sentiment)) %>%
arrange(desc(grandtotal_sentiment))
View(mb_words_by_author_agg_sentiment)
View(mb_cleaned)
View(mb_messages_by_author)
View(mb_words_by_author)
mb_words_by_author_NotInDict <- mb_words_by_author %>%
filter(in_dictionary > 0) %>%
group_by(author) %>%
summarise(sum(in_dictionary), data = list(word))
View(mb_words_by_author_NotInDict)
View(mb_words_by_author_agg_sentiment)
# Aggregate sentiments to get total author sentiment score
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
group_by(author) %>%
group_nest() %>%
summarise(grandtotal_sentiment = sum(total_sentiment)) %>%
arrange(desc(grandtotal_sentiment))
# Aggregate sentiments to get total author sentiment score
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
group_by(author) %>%
group_nest()
View(mb_words_by_author_agg_sentiment[[2]][[1]])
# Aggregate sentiments to get total author sentiment score
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
nest_by(author) %>%
summarise(grandtotal_sentiment = sum(total_sentiment)) %>%
arrange(desc(grandtotal_sentiment))
# Aggregate sentiments to get total author sentiment score
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
chop(author) %>%
#summarise(grandtotal_sentiment = sum(total_sentiment)) %>%
#arrange(desc(grandtotal_sentiment))
# Aggregate words not found in GradyAugmented dataset and group by author
mb_words_by_author_NotInDict <- mb_words_by_author %>%
filter(in_dictionary > 0) %>%
group_by(author) %>%
summarise(sum(in_dictionary), data = list(word))
# Aggregate sentiments to get total author sentiment score
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
chop(author)
View(mb_messages_by_author)
View(mb_words_by_author)
# Aggregate sentiments to get total author sentiment score
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
chop(author)
View(mb_words_by_author_agg_sentiment)
# Aggregate sentiments to get total author sentiment score
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
chop(c(word, n, in_dictionary, sentiment, total_sentiment))
View(mb_words_by_author_agg_sentiment)
# Aggregate sentiments to get total author sentiment score
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
chop(c(word, n, in_dictionary, sentiment, total_sentiment)) %>%
mutate(grandtotal_sentiment = sum(total_sentiment))
# Aggregate sentiments to get total author sentiment score
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
chop(c(word, n, in_dictionary, sentiment, total_sentiment)) %>%
rowwise() %>%
mutate(grandtotal_sentiment = sum(total_sentiment))
mb_words_by_author_agg_sentiment <- select(mb_words_by_author_agg_sentiment - c(n, in_dictionary, sentiment))
mb_words_by_author_agg_sentiment <- select(mb_words_by_author_agg_sentiment - c("n", "in_dictionary", "sentiment"))
mb_words_by_author_agg_sentiment <- select(mb_words_by_author_agg_sentiment, -c(n, in_dictionary, sentiment))
mb_words_by_author_agg_sentiment <- select(mb_words_by_author_agg_sentiment, -c(total_sentiment))
# Filter sentiment score to above 1.50
mb_words_by_author_agg_sentiment <- mb_words_by_author_agg_sentiment %>%
filter(grandtotal_sentiment > 1.5)
# Save high sentiment authors
high_sentiment_authors <- mb_words_by_author_agg_sentiment$author
ggplot(mb_words_by_author_agg_sentiment, aes(x=author, y=grandtotal_sentiment))+
geom_line()
ggplot(mb_words_by_author_agg_sentiment, aes(x=author, y=grandtotal_sentiment))+
geom_bar()
ggplot(mb_words_by_author_agg_sentiment, aes(author,grandtotal_sentiment))+
geom_bar()
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
group_nest() %>%
summarise(grandtotal_sentiment = sum(total_sentiment)) %>%
arrange(desc(grandtotal_sentiment))
View(mb_words_by_author)
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
group_nest() %>%
summarise(grandtotal_sentiment = sum(total_sentiment)) %>%
arrange(desc(grandtotal_sentiment))
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
group_nest(author) %>%
summarise(grandtotal_sentiment = sum(total_sentiment)) %>%
arrange(desc(grandtotal_sentiment))
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
group_by(author) %>%
summarise(grandtotal_sentiment = sum(total_sentiment)) %>%
arrange(desc(grandtotal_sentiment))
View(mb_messages_by_author_aggregated)
View(mb_words_by_author_agg_sentiment)
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
group_by(author) %>%
summarise(grandtotal_sentiment = sum(total_sentiment)) %>%
arrange(desc(grandtotal_sentiment)) %>%
filter(grandtotal_sentiment > 1.5)
ggplot(mb_words_by_author_agg_sentiment, aes(author,grandtotal_sentiment))+
geom_bar()
ggplot(mb_words_by_author_agg_sentiment, aes(x=author,y=grandtotal_sentiment))+
geom_bar()
ggplot(mb_words_by_author_agg_sentiment, aes(x=author,y=grandtotal_sentiment))+
geom_scatter()
ggplot(mb_words_by_author_agg_sentiment, aes(x=author,y=grandtotal_sentiment))+
geom_point()
ggplot(mb_words_by_author_agg_sentiment, aes(x=grandtotal_sentiment,y=author))+
geom_point()
ggplot(mb_words_by_author_agg_sentiment, aes(x=grandtotal_sentiment,y=author))+
geom_bar()
ggplot(mb_words_by_author_agg_sentiment, aes(x=grandtotal_sentiment,y=author))+
geom_line()
ggplot(mb_words_by_author_agg_sentiment, aes(x=grandtotal_sentiment,y=author))+
geom_col()
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
group_by(author) %>%
summarise(grandtotal_sentiment = sum(total_sentiment)) %>%
filter(grandtotal_sentiment > 1.5) %>%
arrange(desc(grandtotal_sentiment))
library(data.table)
View(mb_cleaned)
library(dtplyr)
# Save high sentiment authors
high_sentiment_authors <- c("POK", "Viktor-E", "SaveOurWildlands", "maha_Homeland")
DT <- data.table(mb_cleaned)
high_sentiment_dt <- lazy_dt(DT)
high_sentiment_dt %>% filter(author %in% goodHosp) -> filtrate
filtrate
DT <- data.table(mb_cleaned)
high_sentiment_dt <- lazy_dt(DT)
high_sentiment_dt %>% filter(author %in% high_sentiment_authors) -> filtrate
filtrate
high_sentiment_dt %>% filter(author %in% high_sentiment_authors) -> filtrate
datatable(filtrate)
high_sentiment_dt <- mb_cleaned %>% filter(author %in% high_sentiment_authors)
datatable(high_sentiment_dt)
remove(DT)
remove(filtrate)
remove(high_sentiment_dt)
high_sentiment_dt <- mb_cleaned %>%
filter(author %in% high_sentiment_authors)
detach("package:dtplyr", unload = TRUE)
high_sentiment_dt <- mb_cleaned %>%
filter(author %in% high_sentiment_authors)
remove(high_sentiment_dt)
high_sentiment_dt <- mb_cleaned %>%
filter(author %in% high_sentiment_authors)
high_sentiment_dt <- mb_cleaned %>%
filter(author == "POk")
high_sentiment_dt <- raw_data %>%
filter(author == "POk")
detach("package:DT", unload = TRUE)
library(DT)
detach("package:data.table", unload = TRUE)
remove(high_sentiment_dt)
high_sentiment_dt <- raw_data %>%
filter(author == "POk")
remove(high_sentiment_dt)
high_sentiment_dt <- raw_data %>%
filter(author == "POk")
mb_words_by_author_nested <- mb_words_by_author %>%
group_by(author) %>%
group_nest()
knitr::opts_chunk$set(echo = FALSE, message = FALSE, error = FALSE, warning = FALSE, fig.retina = 3)
packages = c('igraph','tidygraph','ggraph','hms','textplot','tidyverse','lubridate','ggwordcloud','DT','wordcloud',
'widyr','tidytext','dplyr','clock','visNetwork','hms','qdapDictionaries',
'corpus','syuzhet','textdata','plotly','patchwork','Rcpp','greekLetters')
for(p in packages){
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
`%notin%` <- Negate(`%in%`)
remove(high_sentiment_dt)
mb_words_by_timeblock <- mb_words %>%
count(time_block, word, sort=TRUE)
library(dtplyr)
detach("package:dtplyr", unload = TRUE)
mb_words_by_timeblock <- mb_words %>%
count(time_block, word, sort=TRUE)
detach("package:dplyr", unload = TRUE)
library(dplyr)
mb_words_by_timeblock <- mb_words %>%
count(time_block, word, sort=TRUE)
View(mb_words_by_author_agg_sentiment)
mb_words_by_timeblock_tfidf <- mb_words_by_timeblock %>%
bind_tf_idf(word, time_block, n) %>%
arrange(desc(tf_idf))
mb_words_by_author_tfidf <- mb_words_by_author %>%
bind_tf_idf(word, author, n) %>%
arrange(desc(tf_idf))
mb_words_by_timeblock <- mb_words %>%
count(time_block, word, sort=TRUE)
high_sentiment_dt <- mb_cleaned %>% filter(author == "POK")
high_sentiment_dt <- mb_cleaned %>% filter(author = "POK")
datatable(high_sentiment_dt)
remove.packages("dtplyr", lib="~/R/win-library/4.0")
knitr::opts_chunk$set(echo = FALSE, message = FALSE, error = FALSE, warning = FALSE, fig.retina = 3)
packages = c('igraph','tidygraph','ggraph','hms','textplot','tidyverse','lubridate','ggwordcloud','DT','wordcloud',
'widyr','tidytext','dplyr','clock','visNetwork','hms','qdapDictionaries',
'corpus','syuzhet','textdata','plotly','patchwork','Rcpp','greekLetters')
for(p in packages){
if(!require(p, character.only = T)){
install.packages(p)
}
library(p, character.only = T)
}
`%notin%` <- Negate(`%in%`)
mctext_files <- list.files(path = "data", pattern = '.csv$', full.names = TRUE) %>%
map(read_csv)
raw_data <- data.table::rbindlist(mctext_files, use.names = TRUE, idcol = "source")
write_rds(raw_data, "data/rds/raw_data.rds")
glimpse(raw_data)
# Extract time
raw_data$timestamp <- paste(substring(raw_data$`date(yyyyMMddHHmmss)`,first = 9, last = 10),
substring(raw_data$`date(yyyyMMddHHmmss)`,first = 11, last = 12),sep=":")
# Extract date
raw_data$date <- paste(substring(raw_data$`date(yyyyMMddHHmmss)`, first = 1, last = 4),
substring(raw_data$`date(yyyyMMddHHmmss)`, first = 5, last = 6),
substring(raw_data$`date(yyyyMMddHHmmss)`, first = 7, last = 8),sep = "-")
# Parse date and time format
raw_data$date <- parse_date(raw_data$date, format = "%Y-%m-%d")
raw_data$timestamp <- parse_time(raw_data$timestamp, format = "%AT")
# Create datetime variable and parse to correct format
raw_data$datetime <- paste(raw_data$date, raw_data$timestamp, sep = " ")
raw_data$datetime <- parse_date_time(raw_data$datetime, "%Y-%m-%d %H:%M:%S")
raw_data$timestamp <- format(as.POSIXct(raw_data$datetime), format = "%H:%M:%S")
# Create new column to assign each message to a particular 30-min period
raw_data$time_block = cut(raw_data$datetime, breaks = "30 min")
levels(raw_data$time_block) <- c("5-5.30pm", "5.30-6pm", "6-6.30pm", "6.30-7pm", "7-7.30pm", "7.30-8pm", "8-8.30pm", "8.30-9pm", "9-9.30pm", "9.30-10pm")
# Drop original datetime column, and source file column
raw_data <- select(raw_data, -c(1,3))
glimpse(raw_data)
# Create list of unique authors
author_list <- unique(raw_data$author)
# Extract hashtags
raw_data$hashtags <- str_extract_all(raw_data$message,"(?<=#)\\w+\\b")
raw_data$hashtags[lengths(raw_data$hashtags) == 0] <- NA_character_
raw_data$hashtags <- as.character(raw_data$hashtags)
# Extract retweet source
raw_data$retweet_source <- str_extract_all(raw_data$message,"(?<=RT @)\\S+\\b")
raw_data$retweet_source[lengths(raw_data$retweet_source) == 0] <- NA_character_
raw_data$retweet_source <- as.character(raw_data$retweet_source)
# Create cleaned_message column without hashtags and retweet sources
raw_data$cleaned_message <- str_replace_all(raw_data$message,"RT @\\S+\\s*\\b","")
raw_data$cleaned_message <- str_replace_all(raw_data$cleaned_message,"#\\w+\\s*\\b","")
# Extract valid (i.e. must be a user in unique author list) mentions from cleaned_message above.
raw_data$mentions <- ifelse(as.character(str_extract_all(raw_data$cleaned_message,"(?<=@)\\S+\\b")) %in% author_list, str_extract_all(raw_data$cleaned_message,"(?<=@)\\S+\\b"),ifelse (as.character(str_extract_all(raw_data$cleaned_message,"(?<=@)\\w+\\b")) %in% author_list,str_extract_all(raw_data$cleaned_message,"(?<=@)\\w+\\b"),NA_character_))
raw_data$mentions <- as.character(raw_data$mentions)
raw_data <- raw_data %>% mutate(mentions = ifelse(as.character(mentions) != as.character(author),mentions,NA_character_))
raw_data$mentions[lengths(raw_data$mentions) == 0] <- NA_character_
# Update cleaned_message to remove mentioned users
raw_data$cleaned_message <- ifelse(as.character(str_extract_all(raw_data$cleaned_message,"(?<=@)\\S+\\b")) %in% author_list | as.character(str_extract_all(raw_data$cleaned_message,"(?<=@)\\w+\\b")) %in% author_list, str_replace_all(raw_data$cleaned_message,"@\\S+\\s*\\b",""), str_replace_all(raw_data$cleaned_message,"@",""))
# Rearrange columns for better viewing
raw_data <- raw_data %>% relocate(mentions, .before = cleaned_message)
raw_data <- raw_data %>% relocate(c(4,5,6), .after = cleaned_message)
# Split mb and cc data for convenient data wrangling later
mb <- raw_data %>% filter(type == "mbdata")
cc <- raw_data %>% filter(type == "ccdata")
glimpse(raw_data)
mb_by_time <- mb %>%
group_by(timestamp) %>%
summarize(mb_count = n())
mb_by_timeblock <- mb %>%
group_by(time_block) %>%
summarize(mb_count = n())
mb_by_timeblock %>%
plot_ly(x = ~time_block, y= ~mb_count) %>%
add_bars()
cc <- cc %>%
mutate(case_type = case_when(
grepl("SUSPICIOUS", cc$message) ~ "Suspicious Check",
grepl("FIRE", cc$message) ~ "Fire related",
grepl("VEHICLE", cc$message) ~ "Vehicle related",
grepl("CROWD", cc$message) ~ "Crowd Control",
grepl("PURSUIT", cc$message) ~ "Pursuit",
grepl("RECKLESS", cc$message) ~ "Pursuit",
grepl("FELONY", cc$message) ~ "Felony",
grepl("CRIME SCENE INVESTIGATION", cc$message) ~ "CSI",
grepl("DIRE", cc$message) ~ "Dire Emergency",
grepl("MISDEMEANOR", cc$message) ~ "Misdemeanor",
grepl("CHECK", cc$message) ~ "Checks",
grepl("STOP", cc$message) ~ "Stops",
TRUE ~ "Others"))
cc <- cc %>%
relocate(case_type, .before = message)
casetype_by_timeblock <- cc %>%
group_by(case_type, time_block) %>%
summarize(case_type_count=n())
casetype_by_timeblock %>%
plot_ly(x = ~time_block, y = ~case_type_count, color = ~case_type) %>%
add_bars() %>%
layout(barmode = "stack")
casetype_by_timeblock %>%
filter(case_type %notin% c("Checks", "Stops", "Others")) %>%
plot_ly(x = ~time_block, y = ~case_type_count, color = ~case_type, colors="Set1") %>%
add_bars() %>%
layout(barmode = "stack")
mb_messages_by_author <- mb %>%
count(author, cleaned_message)
mb_messages_by_author_aggregated <- mb_messages_by_author %>%
group_by(author) %>%
summarize(unique_count = n(), total_count = sum(n), spam_check = total_count/unique_count) %>%
filter(spam_check > 1) %>%
arrange(desc(spam_check))
spam_authors_ordered <- mb_messages_by_author_aggregated$author
datatable(mb_messages_by_author_aggregated)
mb_messages_by_author_aggregated %>%
filter(spam_check > 1.1) %>%
plot_ly(x=~author,y=~spam_check, type = "bar") %>%
layout(xaxis=list(categoryorder = "array", categoryarray=mb_messages_by_author_aggregated$author))
mb_cleaned <- mb[mb$author!="KronosQuoth" & mb$author!="Clevvah4Evah"]
mb_words <- mb_cleaned %>%
unnest_tokens(word, cleaned_message, drop = FALSE) %>%
filter(str_detect(word, "[a-z']$"),
!word %in% stop_words$word)
mb_words <- select(mb_words, -c(type))
# Group words by author
mb_words_by_author <- mb_words %>%
count(author, word, sort = TRUE) %>%
ungroup()
# Create new columns to get sentiments and find out whether the word exists in a dictionary
mb_words_by_author$in_dictionary <- if_else (mb_words_by_author$word %in% GradyAugmented,0,1)
mb_words_by_author$sentiment <- get_sentiment(mb_words_by_author$word, method = "syuzhet")
mb_words_by_author$total_sentiment <- mb_words_by_author$sentiment*mb_words_by_author$n
# Aggregate sentiments to get total author sentiment score
#mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
#chop(c(word, n, in_dictionary, sentiment, total_sentiment)) %>%
#rowwise() %>%
#mutate(grandtotal_sentiment = sum(total_sentiment)) %>%
#filter(grandtotal_sentiment > 1.5)
mb_words_by_author_agg_sentiment <- select(mb_words_by_author_agg_sentiment, -c(n, in_dictionary, sentiment, total_sentiment))
# Group words by author
mb_words_by_author <- mb_words %>%
count(author, word, sort = TRUE) %>%
ungroup()
# Create new columns to get sentiments and find out whether the word exists in a dictionary
mb_words_by_author$in_dictionary <- if_else (mb_words_by_author$word %in% GradyAugmented,0,1)
mb_words_by_author$sentiment <- get_sentiment(mb_words_by_author$word, method = "syuzhet")
mb_words_by_author$total_sentiment <- mb_words_by_author$sentiment*mb_words_by_author$n
# Aggregate sentiments to get total author sentiment score
#mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
#chop(c(word, n, in_dictionary, sentiment, total_sentiment)) %>%
#rowwise() %>%
#mutate(grandtotal_sentiment = sum(total_sentiment)) %>%
#filter(grandtotal_sentiment > 1.5)
#mb_words_by_author_agg_sentiment <- select(mb_words_by_author_agg_sentiment, -c(n, in_dictionary, sentiment, total_sentiment))
mb_words_by_author_agg_sentiment <- mb_words_by_author %>%
group_by(author) %>%
summarise(grandtotal_sentiment = sum(total_sentiment)) %>%
filter(grandtotal_sentiment > 1.5) %>%
arrange(desc(grandtotal_sentiment))
# Save high sentiment authors
high_sentiment_authors <- c("POK", "Viktor-E", "SaveOurWildlands", "maha_Homeland")
# Aggregate words not found in GradyAugmented dataset and group by author
mb_words_by_author_NotInDict <- mb_words_by_author %>%
filter(in_dictionary > 0) %>%
group_by(author) %>%
summarise(sum(in_dictionary), data = list(word))
ggplot(mb_words_by_author_agg_sentiment, aes(x=grandtotal_sentiment,y=author))+
geom_col()
high_sentiment_dt <- mb_cleaned %>% filter(author %in% high_sentiment_authors)
install.packages("dtplyr")
datatable(high_sentiment_dt)
high_sentiment_dt <- mb_cleaned %>%
filter(author %in% high_sentiment_authors) %>%
transmute(author=author, message=message)
datatable(high_sentiment_dt)
View(mb_words_by_author_NotInDict)
# Aggregate words not found in GradyAugmented dataset and group by author
mb_words_by_author_NotInDict <- mb_words_by_author %>%
filter(in_dictionary > 0) %>%
group_by(author) %>%
summarise(not_words_count=sum(in_dictionary), data = list(word))
# Aggregate words not found in GradyAugmented dataset and group by author
mb_words_by_author_NotInDict <- mb_words_by_author %>%
filter(in_dictionary > 0) %>%
group_by(author) %>%
summarise(not_words_count=sum(in_dictionary), data = list(word)) %>%
arrange(desc(not_words_count))
# Aggregate words not found in GradyAugmented dataset and group by author
mb_words_by_author_NotInDict <- mb_words_by_author %>%
filter(in_dictionary > 0) %>%
group_by(author) %>%
summarise(not_words_count=sum(in_dictionary), data = list(word)) %>%
arrange(desc(not_words_count)) %>%
filter(not_words_count < 20)
# Aggregate words not found in GradyAugmented dataset and group by author
mb_words_by_author_NotInDict <- mb_words_by_author %>%
filter(in_dictionary > 0) %>%
group_by(author) %>%
summarise(not_words_count=sum(in_dictionary), data = list(word)) %>%
arrange(desc(not_words_count)) %>%
filter(not_words_count > 20)
# Save gibberish authors
gibberish_authors <- mb_words_by_author_NotInDict$author
not_words_dt <- mb_cleaned %>%
filter(author %in% gibberish_authors) %>%
transmute(author=author, message=message)
datatable(not_words_dt)
remove(gibberish_authors)
remove(not_words_dt)
datatable(mb_words_by_author_NotInDict)
Officia1AbilaPost_dt <- mb_cleaned %>%
filter(author == "Officia1AbilaPost") %>%
transmute(author=author, message=cleaned_message)
datatable(Officia1AbilaPost_dt)
wordcloud(mb_words_by_author$word, mb_words_by_author$n, max.words = 300)
View(mb_words_by_author)
set.seed(1234)
wordcloud(mb_words_by_author$word, mb_words_by_author$n, max.words = 300)
View(mb_words)
authors_exclude <- c("POK", "Viktor-E", "SaveOurWildlands", "maha_Homeland", "Officia1AbilaPost")
mb_cleaned2 <- mb_cleaned[mb_cleaned$author %notin% authors_exclude]
View(mb_cleaned2)
mb_words2 <- mb_cleaned2 %>%
unnest_tokens(word, cleaned_message, drop = FALSE) %>%
filter(str_detect(word, "[a-z']$"),
!word %in% stop_words$word)
mb_words2 <- select(mb_words2, -c(type))
View(mb_words2)
# Group words by author
mb_words2_by_author <- mb_words2 %>%
count(author, word, sort = TRUE) %>%
ungroup()
# Create new columns to get sentiments and find out whether the word exists in a dictionary
mb_words2_by_author$in_dictionary <- if_else (mb_words2_by_author$word %in% GradyAugmented,0,1)
mb_words2_by_author$sentiment <- get_sentiment(mb_words2_by_author$word, method = "syuzhet")
mb_words2_by_author$sentiment <- get_sentiment(mb_words2_by_author$word, method = "syuzhet")
mb_words2_by_author$total_sentiment <- mb_words2_by_author$sentiment*mb_words2_by_author$n
mb_words2_by_author_agg_sentiment <- mb_words2_by_author %>%
group_by(author) %>%
summarise(grandtotal_sentiment = sum(total_sentiment)) %>%
filter(grandtotal_sentiment > 1.5) %>%
arrange(desc(grandtotal_sentiment))
mb_by_timeblock %>%
plot_ly(x = ~time_block, y= ~mb_count) %>%
add_bars()
mb_hashtags <- mb %>%
unnest(hashtags, keep_empty = TRUE)
mb_mentions <- mb %>%
filter(!is.na(mentions)) %>%
unnest(mentions, keep_empty = FALSE)
View(mb_hashtags)
View(mb_hashtags)
mb_hashtags <- mb %>%
unnest(hashtags, keep_empty = TRUE)
mb_mentions <- mb %>%
filter(!is.na(mentions)) %>%
unnest(mentions, keep_empty = FALSE)
View(mb_mentions)
mb_hashtags <- mb %>%
unnest(hashtags)
mb_hashtags <- mb %>%
unnest(mb$hashtags)
mb_hashtags <- mb %>%
unnest(mb$hashtags, keep_empty = TRUE)
View(mb)
mb_hashtags <- mb %>% unnest(mb$hashtags, keep_empty = TRUE)
mb_hashtags <- mb %>% unnest(c(hashtags), keep_empty = TRUE)
mb_hashtags <- mb %>% unnest(c(hashtags), keep_empty = TRUE)
View(mb_hashtags)
mb_hashtags <- mb %>% unnest(c(hashtags), keep_empty = TRUE)
View(mb_hashtags)
set.seed(1234)
wordcloud(mb_words_by_author$word, mb_words_by_author$n, max.words = 300)
mb_hashtags <- mb %>% unnest(c(hashtags), keep_empty = TRUE)
View(mb_hashtags)
